{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ¦¥ Unsloth: Will patch your computer to enable 2x faster free finetuning.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-21 12:25:27.108904: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1747815927.124417 1783607 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1747815927.129288 1783607 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "W0000 00:00:1747815927.141888 1783607 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1747815927.141901 1783607 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1747815927.141903 1783607 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1747815927.141904 1783607 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "2025-05-21 12:25:27.145618: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "WARNING[XFORMERS]: xFormers can't load C++/CUDA extensions. xFormers was built for:\n",
      "    PyTorch 2.5.1+cu121 with CUDA 1201 (you have 2.4.1+cu121)\n",
      "    Python  3.10.16 (you have 3.10.15)\n",
      "  Please reinstall xformers (see https://github.com/facebookresearch/xformers#installing-xformers)\n",
      "  Memory-efficient attention, SwiGLU, sparse and more won't be available.\n",
      "  Set XFORMERS_MORE_DETAILS=1 for more details\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ¦¥ Unsloth Zoo will now patch everything to make training faster!\n",
      "Unsloth: If you want to finetune Gemma 2, install flash-attn to make it faster!\n",
      "To install flash-attn, do the below:\n",
      "\n",
      "pip install --no-deps --upgrade \"flash-attn>=2.6.3\"\n",
      "==((====))==  Unsloth 2025.5.6: Fast Gemma2 patching. Transformers: 4.51.3.\n",
      "   \\\\   /|    NVIDIA GeForce RTX 4090. Num GPUs = 1. Max memory: 23.64 GB. Platform: Linux.\n",
      "O^O/ \\_/ \\    Torch: 2.4.1+cu121. CUDA: 8.9. CUDA Toolkit: 12.1. Triton: 3.0.0\n",
      "\\        /    Bfloat16 = TRUE. FA [Xformers = None. FA2 = False]\n",
      " \"-____-\"     Free license: http://github.com/unslothai/unsloth\n",
      "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Gemma2ForCausalLM(\n",
       "  (model): Gemma2Model(\n",
       "    (embed_tokens): Embedding(256000, 3584, padding_idx=0)\n",
       "    (layers): ModuleList(\n",
       "      (0-41): 42 x Gemma2DecoderLayer(\n",
       "        (self_attn): Gemma2Attention(\n",
       "          (q_proj): Linear4bit(in_features=3584, out_features=4096, bias=False)\n",
       "          (k_proj): Linear4bit(in_features=3584, out_features=2048, bias=False)\n",
       "          (v_proj): Linear4bit(in_features=3584, out_features=2048, bias=False)\n",
       "          (o_proj): Linear4bit(in_features=4096, out_features=3584, bias=False)\n",
       "          (rotary_emb): GemmaFixedRotaryEmbedding()\n",
       "        )\n",
       "        (mlp): Gemma2MLP(\n",
       "          (gate_proj): Linear4bit(in_features=3584, out_features=14336, bias=False)\n",
       "          (up_proj): Linear4bit(in_features=3584, out_features=14336, bias=False)\n",
       "          (down_proj): Linear4bit(in_features=14336, out_features=3584, bias=False)\n",
       "          (act_fn): PytorchGELUTanh()\n",
       "        )\n",
       "        (input_layernorm): Gemma2RMSNorm((3584,), eps=1e-06)\n",
       "        (post_attention_layernorm): Gemma2RMSNorm((3584,), eps=1e-06)\n",
       "        (pre_feedforward_layernorm): Gemma2RMSNorm((3584,), eps=1e-06)\n",
       "        (post_feedforward_layernorm): Gemma2RMSNorm((3584,), eps=1e-06)\n",
       "      )\n",
       "    )\n",
       "    (norm): Gemma2RMSNorm((3584,), eps=1e-06)\n",
       "    (rotary_emb): GemmaFixedRotaryEmbedding()\n",
       "  )\n",
       "  (lm_head): Linear(in_features=3584, out_features=256000, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# model_name = 'GoToCompany/gemma2-9b-cpt-sahabatai-v1-base'\n",
    "model_name = \"google/gemma-2-9b-it\"\n",
    "from unsloth import FastLanguageModel\n",
    "test_language = 'su'\n",
    "import os\n",
    "import re\n",
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name = model_name,\n",
    "    max_seq_length = 8192,\n",
    "    load_in_4bit = True,\n",
    "    # token = \"hf_...\"\n",
    ")\n",
    "\n",
    "FastLanguageModel.for_inference(model) # Enable native 2x faster inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e77bac07282541a68cbd55bd3df74102",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1094 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "data_path = f\"../../dataset/test/test_{test_language}.csv\"\n",
    "def load_test_data(data_path):\n",
    "    data = pd.read_csv(data_path)\n",
    "    contexts = []\n",
    "    endings = []\n",
    "    EOS_TOKEN = tokenizer.eos_token\n",
    "    for idx, row in data.iterrows():\n",
    "        sents = []\n",
    "        for i in [4,3,2,1]:\n",
    "            current_sentence = row[f'sentence_{i}']\n",
    "            current_sentence = current_sentence + '.' if current_sentence[-1] != '.' else current_sentence\n",
    "            sents.insert(0, current_sentence)\n",
    "        context = ' '.join(sents) \n",
    "        ending = row['correct_ending']\n",
    "        # ending2 = row['incorrect_ending']\n",
    "        contexts.append(context)\n",
    "        endings.append(ending)\n",
    "\n",
    "    # convert to huggingface dataset\n",
    "    from datasets import Dataset\n",
    "    dataset = Dataset.from_dict({'context': contexts, 'ending': endings})\n",
    "    def formatting_prompts_func(examples):\n",
    "        contexts  = examples[\"context\"]\n",
    "        endings = examples[\"ending\"]\n",
    "        texts = []\n",
    "        for context, ending in zip(contexts, endings):\n",
    "            chat = [\n",
    "                {\"role\": \"user\", \"content\": f\"Generate the ending for the following given story context\\nStory Context: {context}\\nProvide the ending in a single sentence. Don't add anything else.\"},\n",
    "            ]\n",
    "            text = tokenizer.apply_chat_template(chat, tokenize=False, add_generation_prompt=True)\n",
    "            # text = alpaca_prompt.format(context)\n",
    "            texts.append(text)\n",
    "        return { \"text\" : texts, }\n",
    "    dataset = dataset.map(formatting_prompts_func, batched = True,)\n",
    "    return dataset\n",
    "dataset = load_test_data(data_path)\n",
    "contexts = dataset['text']\n",
    "correct_endings = dataset['ending']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "394ab21273ef4149acfd17b98f8a80fe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "AUTOTUNE bmm(256x146x256, 256x256x146)\n",
      "  triton_bmm_45 0.0727 ms 100.0%\n",
      "  triton_bmm_39 0.0737 ms 98.6%\n",
      "  triton_bmm_44 0.0737 ms 98.6%\n",
      "  triton_bmm_51 0.0748 ms 97.3%\n",
      "  triton_bmm_40 0.0748 ms 97.2%\n",
      "  triton_bmm_47 0.0756 ms 96.1%\n",
      "  triton_bmm_41 0.0758 ms 95.9%\n",
      "  triton_bmm_49 0.0758 ms 95.9%\n",
      "  bmm 0.0768 ms 94.7%\n",
      "  triton_bmm_48 0.0778 ms 93.4%\n",
      "SingleProcess AUTOTUNE benchmarking takes 1.9436 seconds and 0.0077 seconds precompiling\n",
      "AUTOTUNE bmm(256x146x146, 256x146x256)\n",
      "  triton_bmm_66 0.0696 ms 100.0%\n",
      "  triton_bmm_73 0.0748 ms 93.2%\n",
      "  triton_bmm_68 0.0758 ms 91.9%\n",
      "  triton_bmm_67 0.0767 ms 90.8%\n",
      "  triton_bmm_59 0.0778 ms 89.5%\n",
      "  triton_bmm_63 0.0778 ms 89.5%\n",
      "  triton_bmm_62 0.0809 ms 86.1%\n",
      "  triton_bmm_64 0.0850 ms 81.9%\n",
      "  triton_bmm_70 0.0850 ms 81.9%\n",
      "  triton_bmm_71 0.0870 ms 80.0%\n",
      "SingleProcess AUTOTUNE benchmarking takes 2.0481 seconds and 0.0011 seconds precompiling\n",
      "/home/rifo.genadi/.conda/envs/local_cloze/lib/python3.10/site-packages/unsloth/kernels/utils.py:443: UserWarning: An output with one or more elements was resized since it had shape [1, 16, 3584], which does not match the required output shape [16, 1, 3584]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)\n",
      "  out = torch_matmul(X, W, out = out)\n"
     ]
    }
   ],
   "source": [
    "from tqdm.auto import tqdm\n",
    "import torch\n",
    "\n",
    "context = contexts[]\n",
    "\n",
    "inputs = tokenizer(batch_contexts, return_tensors=\"pt\", truncation=True).to(\"cuda\")\n",
    "\n",
    "all_predictions = []\n",
    "\n",
    "for i in tqdm(range(0, len(batch_contexts), batch_size)):\n",
    "    input_ids_batch = inputs['input_ids'][i:i+batch_size]\n",
    "    attention_mask_batch = inputs['attention_mask'][i:i+batch_size]\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        res = model.generate(\n",
    "            input_ids=input_ids_batch,\n",
    "            attention_mask=attention_mask_batch,\n",
    "            max_new_tokens=128\n",
    "        )\n",
    "    \n",
    "    # Decode only the generated part\n",
    "    for j, output in enumerate(res):\n",
    "        input_len = (input_ids_batch[j] != tokenizer.pad_token_id).sum()\n",
    "        generated = output[input_len:]\n",
    "    decoded = tokenizer.decode(generated, skip_special_tokens=True).strip()\n",
    "    all_predictions.append(decoded)\n",
    "\n",
    "# `all_predictions` now contains the generated endings for each context\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compute BertScore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([     2,      2,    106,   1645,    108,  38557,    573,  16851,    604,\n",
       "           573,   2412,   2764,   3904,   4807,    108,  21519,  19422, 235292,\n",
       "         37747, 235259,   1957,   1063,   4221,    569,   1465,    553,  20953,\n",
       "        114317,   1966,   1569, 235248, 235284, 235308,   3586,    549, 235265,\n",
       "          4378,   1569, 235248, 235284, 235304,   3586,    549,  60068,   1021,\n",
       "        235269,    625,  11055,    798,  40471,    682,   4280,    575,   9732,\n",
       "         31678,   1577,  40608, 235250, 235265,    584,  47405,  99403,   8550,\n",
       "           966, 235269,  31678,   1577,  59764,  51730,    655,   1021,    575,\n",
       "          9732,    556,    693,   2998, 235265,   1881,   1063,    556,    493,\n",
       "           541,  59764,   8227, 148152, 211157,  99403,   5675,   2624, 235259,\n",
       "        144921, 207099,    699,   3068, 235265,    108,  70029,    573,  16851,\n",
       "           575,    476,   3821,  13060, 235265,   4257, 235303, 235251,   1843,\n",
       "          4341,   1354, 235265,    107,    108,    106,   2516,    108,      0,\n",
       "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
       "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
       "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
       "             0,      0], device='cuda:0')"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "unsloth_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
